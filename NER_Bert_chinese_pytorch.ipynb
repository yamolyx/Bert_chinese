{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTmNj95EJub3"
   },
   "source": [
    "# Named Entity Recognition with BERT in PyTorch\n",
    "Based: https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a  \n",
    "library: Hugging face's \"Transformer\"   \n",
    "Êï∞ÊçÆÂ∫ìÂ≠òÂú®ËÑèÊï∞ÊçÆÔºÅlabelÂíåËØçÈïøÂ∫¶‰∏ç‰∏ÄËá¥  \n",
    "We use \"BertFortokenClassification\" instead of \"BertForSequenceClassification\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Mode chosing\n",
    "Colab=False"
   ],
   "metadata": {
    "id": "f_akTVv8EAkx"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#######For google\n",
    "if Colab:\n",
    "    !pip install transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3Ycr6F5J76T",
    "outputId": "76de84d2-789a-491b-f3e0-22e91d9a488f"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "#if we use import torch.utils.data.DataLoader as Dataloader, here dataloader is a module\n",
    "#but here, from torch,utils.data import dataloader is now a function\n",
    "import torch.optim as optim   #Here optim is still a module, we always use optim.SGD to create a function SGD\n",
    "from tqdm import tqdm   #same reason, if we \"import tqdm\" directly, will be error: module is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "########Load from gdrive\n",
    "if Colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Input/People_dalily_10000_examples_adapted.csv\")\n",
    "\n",
    "\n",
    "####load(local) \n",
    "else:\n",
    "    df=pd.read_csv('./People_dalily_10000_examples_adapted.csv')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_XQJrOzKfgp",
    "outputId": "4115bd42-5fcf-468c-b5f0-02a7be60da34"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9slZGLglJub-",
    "outputId": "b1886154-6d06-4013-8da9-09427c46173e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  ‰∫∫ Ê∞ë ÁΩë 1 Êúà 1 Êó• ËÆØ ÊçÆ „Ää Á∫Ω Á∫¶ Êó∂ Êä• „Äã Êä• ÈÅì , Áæé ÂõΩ Âçé Â∞î Ë°ó ...   \n1  „Ää Á∫Ω Á∫¶ Êó∂ Êä• „Äã Êä• ÈÅì ËØ¥ , Ê†á ÊôÆ 5 0 0 Êåá Êï∞ ‰ªä Âπ¥ ‰∏ä Âçá 2 9 ...   \n2  Â∞± 1 2 Êúà 3 1 Êó• Êù• ËØ¥ , Áî± ‰∫é Â∞± ‰∏ö Ââç ÊôØ Áúã Â•Ω Âíå Áªè Êµé Â¢û Èïø ...   \n3  Âè¶ ÊçÆ „Ää Âçé Â∞î Ë°ó Êó• Êä• „Äã Êä• ÈÅì , 2 0 1 3 Âπ¥ ÊòØ 1 9 9 5 Âπ¥ ...   \n4  ‰∫∫ Ê∞ë ÁΩë Âπ≥ Â£§ 1 Êúà 1 Êó• Áîµ ( ËÆ∞ ËÄÖ Áéã Ëéâ „ÄÅ Á®ã Áª¥ ‰∏π ) Êúù È≤ú ÊúÄ ...   \n\n                                              labels  \n0  O O O B_T I_T I_T I_T O O O B_LOC I_LOC O O O ...  \n1  O B_LOC I_LOC O O O O O O O O O O O O O O B_T ...  \n2  O B_T I_T I_T I_T I_T I_T O O O O O O O O O O ...  \n3  O O O B_LOC I_LOC I_LOC O O O O O O B_T I_T I_...  \n4  O O O B_LOC I_LOC I_T I_T I_T I_T O O O O B_PE...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>‰∫∫ Ê∞ë ÁΩë 1 Êúà 1 Êó• ËÆØ ÊçÆ „Ää Á∫Ω Á∫¶ Êó∂ Êä• „Äã Êä• ÈÅì , Áæé ÂõΩ Âçé Â∞î Ë°ó ...</td>\n      <td>O O O B_T I_T I_T I_T O O O B_LOC I_LOC O O O ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>„Ää Á∫Ω Á∫¶ Êó∂ Êä• „Äã Êä• ÈÅì ËØ¥ , Ê†á ÊôÆ 5 0 0 Êåá Êï∞ ‰ªä Âπ¥ ‰∏ä Âçá 2 9 ...</td>\n      <td>O B_LOC I_LOC O O O O O O O O O O O O O O B_T ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Â∞± 1 2 Êúà 3 1 Êó• Êù• ËØ¥ , Áî± ‰∫é Â∞± ‰∏ö Ââç ÊôØ Áúã Â•Ω Âíå Áªè Êµé Â¢û Èïø ...</td>\n      <td>O B_T I_T I_T I_T I_T I_T O O O O O O O O O O ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Âè¶ ÊçÆ „Ää Âçé Â∞î Ë°ó Êó• Êä• „Äã Êä• ÈÅì , 2 0 1 3 Âπ¥ ÊòØ 1 9 9 5 Âπ¥ ...</td>\n      <td>O O O B_LOC I_LOC I_LOC O O O O O O B_T I_T I_...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>‰∫∫ Ê∞ë ÁΩë Âπ≥ Â£§ 1 Êúà 1 Êó• Áîµ ( ËÆ∞ ËÄÖ Áéã Ëéâ „ÄÅ Á®ã Áª¥ ‰∏π ) Êúù È≤ú ÊúÄ ...</td>\n      <td>O O O B_LOC I_LOC I_T I_T I_T I_T O O O O B_PE...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df=pd.read_csv('ner.csv')   #Not completed, like \"Demonstrators\" not regard as ent\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrGeBoY6JucA"
   },
   "source": [
    "## Give ids to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bK_pZSJmJucB",
    "outputId": "98a8fc5e-18ed-4a95-dfc7-a41b07afb3bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B_LOC', 1: 'B_ORG', 2: 'B_PER', 3: 'B_T', 4: 'I_LOC', 5: 'I_ORG', 6: 'I_PER', 7: 'I_T', 8: 'O'}\n"
     ]
    }
   ],
   "source": [
    "# Split labels based on whitespace and turn them into a list\n",
    "labels = [i.split() for i in df['labels'].values.tolist()]\n",
    "\n",
    "# Check how many labels are there in the dataset\n",
    "unique_labels = set()\n",
    "\n",
    "for lb in labels:\n",
    "  [unique_labels.add(i) for i in lb if i not in unique_labels]   #here .add is fun without return value, thus if we set a=[...], \n",
    "                                                                  # a will =[]\n",
    "\n",
    "# Map each label into its id representation and vice versa\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "print(ids_to_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf0A1ZezJucC"
   },
   "source": [
    "## Tokenize(Output directly become tensor)\n",
    "Bert tokenizer can transform a sentence in nl to a list of number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "51TrjL5aJucC",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "e065e60755f6426186019993cd83f68b",
      "ad8c77c860f6481581e370f21d6b8621",
      "3a7e5e1b461d4356adddbc0cc2230f12",
      "c71096b610684c5ca9a688abac7b93ab",
      "cd28f3f9c1fe4ae6b4422f908ca832dd",
      "fd8269fd57e6488fa779ff9affb4fd79",
      "20446dd83deb41c9980cb04198923221",
      "14e509b410ef4dc29b009bcd3d97ec68",
      "51d8f1fee1db47d7875263cda6d167dc",
      "e7af4cead3e24e329ae30cab1e63bccf",
      "c5ba7995840c4276be6e60cc7a5eab68",
      "c6917aa5edce467b8588326817f31456",
      "b0d498efdd7d41388d584731766857b5",
      "efbc5f11c21a43f6ab34c98ef09bcd1c",
      "bc71afe9bb58448f82671976842e11ab",
      "6164fce756d64776adc4d72dd4a639d1",
      "531846296ec74feda807d6cf16d1b39d",
      "961234984ce74cd2a735a9fb95da4828",
      "097e5374ef9b44f2bd5e165156df56b6",
      "47e6774978c843e9986e538a68721724",
      "cd97bc185757489db76afafa1b21a132",
      "3f4f930c8f5a45be9f243b476e27ac2d",
      "faa046bbf5ce497bbf6f3e7ee8d1a828",
      "9d50b987d9e84e4c97fb11e288814a14",
      "24d1d6b9a112406f904c35f693dfbfd8",
      "242a2025e4744453b4d8c209eb0d478e",
      "c946015d74c748fd9e24ba6e6a285007",
      "6990ee6a22494ec282cc424ba4757a52",
      "cdda716bac6c4de092dfcea20163188a",
      "9d7d54bba2cd48f0bca1e479d8f0367b",
      "cacee7a3bb9343b7bf6c5b069b635ed9",
      "eee57270de964cbaadceb07a8515eb93",
      "27012be861e1432ab4b237e48a0c25f4",
      "8d95c054583946499f2f3e500b72d927",
      "4dd3e3b7ab324cc8ababdd8a447263f8",
      "39867111d6ba404e8db39f783e884543",
      "ca7bab79d925449fbe520e34871192cf",
      "c75453ebeca8488788009c2e9c722553",
      "7d6637412ca641d0a605cf348ba0cc39",
      "9db41e0582514932b56f64ecf337d2ee",
      "25848c0cd7654e9f836c16193307807a",
      "db599e29c05449d6af1f9137de95f940",
      "56928713eec84d3e823110ebac8c87cf",
      "e23a833aad07459bbd42247f84f07638"
     ]
    },
    "outputId": "a94e3430-fa78-437e-c3a0-ea7b332f9aa0"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at how can we preprocess the text - Take first example\n",
    "text = df['text'].values.tolist()\n",
    "example = text[36]   #get one sentence (THE 36th sentence)\n",
    "\n",
    "#tokenize by bert\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "text_tokenized = tokenizer(example, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "#3 layer can be found: input_ids, token_type_ids,atention_mask\n",
    "#print(text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L9Vz6v7JucD"
   },
   "source": [
    "### decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TXFVfD1rJucE"
   },
   "outputs": [],
   "source": [
    "#print(tokenizer.decode(text_tokenized.input_ids[0][0:50]))    #0 is the first sentence,since here we have only one sentence to tokenize\n",
    "#we can't decode more than 1 sentence with .decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcNmzzKiJucF"
   },
   "source": [
    "### \"Convert ids to token\" will find Bert's\"subword\" problem\n",
    "The BERT tokenizer uses the so-called word-piece tokenizer under the hood, which is a sub-word tokenizer. This means that BERT tokenizer will likely to **split one word into one or more meaningful sub-words**.\n",
    "*Which make label provided by database can't match tokens*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oe5YyieKJucH",
    "outputId": "06cad226-eb84-465e-8bd0-99928c62d713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Êñ∞', 'Âπ¥', 'Ââç', 'Â§ï', ',', 'ÂõΩ', 'ÂÆ∂', '‰∏ª', 'Â∏≠', '‰π†', 'Ëøë', 'Âπ≥', 'ÈÄö', 'Ëøá', '‰∏≠', 'ÂõΩ', 'ÂõΩ', 'ÈôÖ', 'Âπø', 'Êí≠', 'Áîµ', 'Âè∞', '„ÄÅ', '‰∏≠', 'Â§Æ', '‰∫∫', 'Ê∞ë', 'Âπø', 'Êí≠', 'Áîµ', 'Âè∞', '„ÄÅ', '‰∏≠', 'Â§Æ', 'Áîµ', 'ËßÜ', 'Âè∞', ',', 'Âèë', 'Ë°®', '‰∫Ü', '2', '0', '1', '4', 'Âπ¥', 'Êñ∞', 'Âπ¥', 'Ë¥∫']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n"
     ]
    }
   ],
   "source": [
    "######Check output of word_ids, we will find it pretty smart:\n",
    "######First token [CLS] is marked as None, which avoid the movement of whole sentence\n",
    "######All words are marked with their real \"index\" in \"labels list\"(0st word is Prime, and 3rd word is G+ei+r)\n",
    "word_ids = text_tokenized.word_ids()\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0])[0:50])\n",
    "print(word_ids[0:50])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ1avpX_JucH"
   },
   "source": [
    "### Two method can be applied to solve this problem:\n",
    "These word_ids will be very useful to adjust the length of the label by applying either of these two methods:  \n",
    "\n",
    "1, We only provide a label to the first sub-word of each splitted token. The continuation of the sub-word then will simply have ‚Äò-100‚Äô as a label. All tokens that don‚Äôt have word_ids will also be labeled with ‚Äò-100‚Äô.  \n",
    "2, We provide the same label among all of the sub-words that belong to the same token. All tokens that don‚Äôt have word_ids will be labeled with ‚Äò-100‚Äô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "hlsdh7_cJucI"
   },
   "outputs": [],
   "source": [
    "def align_label_example(tokenized_input, labels,labels_to_ids,label_all_tokens):\n",
    "    '''\n",
    "    output: labels_ids\n",
    "    \n",
    "    '''\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    #print('word')\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "            \n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])  #As we mention, word_idx is exactly \n",
    "            except:\n",
    "                label_ids.append(-100)   #Ê≠§Â§ÑÊòØÈÅøÂÖçËÑèÊï∞ÊçÆÁöÑÂΩ±ÂìçÔºàÂç≥labelÁöÑÈïøÂ∫¶ÂíåÂÆûÈôÖÁöÑÂè•Â≠êÈïøÂ∫¶‰∏çÂêåÔºâ\n",
    "    \n",
    "        else:\n",
    "            # print('label_id', label_ids)\n",
    "            # print('word_ids ',word_ids[0:50])\n",
    "            # #print('label[word]', labels[word_idx])\n",
    "            # print('label list ', len(labels))\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "    \n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-fUGFacJucJ"
   },
   "source": [
    "## New lables for bert to train\n",
    "Since original labels can't match with its token list, we create a new lables list to fit it  \n",
    "what is more, we present labels in ids form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryHR4S01JucJ",
    "outputId": "46882fb2-dcef-4cb1-ec02-284abc441e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 3, 7, 8, 8, 8, 8, 8, 8, 8, 2, 6, 6, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "['[CLS]', 'Êñ∞', 'Âπ¥', 'Ââç', 'Â§ï', ',', 'ÂõΩ', 'ÂÆ∂', '‰∏ª', 'Â∏≠', '‰π†', 'Ëøë', 'Âπ≥', 'ÈÄö', 'Ëøá', '‰∏≠', 'ÂõΩ', 'ÂõΩ', 'ÈôÖ', 'Âπø', 'Êí≠', 'Áîµ', 'Âè∞', '„ÄÅ', '‰∏≠']\n"
     ]
    }
   ],
   "source": [
    "label = labels[36]\n",
    "\n",
    "#If we set label_all_tokens to True.....\n",
    "label_all_tokens = True\n",
    "\n",
    "new_label = align_label_example(text_tokenized, label,labels_to_ids,label_all_tokens)\n",
    "print(new_label[0:25])   #he\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0])[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZxEVx6GJucK"
   },
   "source": [
    "## Dataset Class(tokenize include)\n",
    "Before we train our BERT model for NER task, we need to create a dataset class to generate and fetch data in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Yj3AkvkmJucK"
   },
   "outputs": [],
   "source": [
    "##### NOt a simple class of dataset, we also realise tokenizer here\n",
    "\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df,labels_to_ids,label_all_tokens):\n",
    "\n",
    "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
    "        txt = df['text'].values.tolist()   #list of sentence\n",
    "        \n",
    "        text_tokenized = [tokenizer(str(i),\n",
    "                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt] \n",
    "        \n",
    "        self.texts=text_tokenized\n",
    "         \n",
    "        self.labels = [align_label_example(i,j,labels_to_ids,label_all_tokens) for i,j in zip(text_tokenized, lb)]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_data, batch_labels   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie32NrQLJucL"
   },
   "source": [
    "### Train val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Jl9k250hJucL"
   },
   "outputs": [],
   "source": [
    "df = df[0:1000]    #we pick only 1000 example\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),    \n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])    #split the lisy into 3 parts, with 2 cut\n",
    "                                                                        #one cut at 0.8*len(df)\n",
    "                                                                        #one cut at 0.9*len(df)\n",
    "                                                                        #so train:val:test=8:1:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdalS2vCJucL"
   },
   "source": [
    "#### Test Datasequence (check with class \"Datasequence\"'s return)\n",
    "Find that output of DataSequence is a list with len of nb_sentence,  and each elements is a tuple  \n",
    "\n",
    "elem 1 in truple : Dict with 3 pairs key-value: input_ids, attention mask, token_type_ids   \n",
    "elem 2 in truple: labels's ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "GvlwvPANJucM"
   },
   "outputs": [],
   "source": [
    "# Data_token=DataSequence(df_train[43:100],labels_to_ids,label_all_tokens)\n",
    "# Data_token.__getitem__(20)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdh9J3ilJucM"
   },
   "source": [
    "### Torch's model definition\n",
    "Define a model class in torch's way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "t1maOjmkJucM"
   },
   "outputs": [],
   "source": [
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertModel, self).__init__()   #for pytorch, this lign is obligatory\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))  #transformer layer\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)  \n",
    "                                                                                    #self.bert equal to model\n",
    "                                                                                    #return_dict=false -> return value is a tuple of (loss, logits)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jysu47ifJucM"
   },
   "source": [
    "### About the warning of BertForTokenClassifica\n",
    "You may occur such warning:   \n",
    "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification:....   \n",
    "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).   \n",
    "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).   \n",
    "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly  initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.   \n",
    "\n",
    "Answer:  \n",
    "https://github.com/huggingface/transformers/issues/5421#issuecomment-652582854  \n",
    "\n",
    "@ohmeow you're loading the **bert-base-cased checkpoint** (which is a checkpoint that was trained using a similar architecture to BertForPreTraining) in a BertForSequenceClassification model.\n",
    "\n",
    "This means that:\n",
    "\n",
    "The layers that BertForPreTraining has, but BertForSequenceClassification does not have will be discarded  \n",
    "The layers that BertForSequenceClassification has but BertForPreTraining does not have will be randomly initialized.  \n",
    "This is expected, and tells you that you won't have good performance with your BertForSequenceClassification model before you   fine-tune it üôÇ."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x=[1,2,3,4]\n",
    "y=torch.tensor(x)\n",
    "y[(y!=1)&(y!=2)]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cv9vUiSof_YU",
    "outputId": "d18a5643-f06f-4c48-968b-a599ae4c1ce8"
   },
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 4])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x=[]\n",
    "y=torch.tensor(x)\n",
    "y.shape[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gjyUZ_inqFh",
    "outputId": "6de29793-7085-43c3-e3ac-33f58aad3a23"
   },
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def acc_calculation(logits,train_label):\n",
    "  acc_batch=[]\n",
    "  nb_valid_label=0\n",
    "  logits_clean = [logits[i,(train_label[i]!=-100)&(train_label[i]!=8)] for i in range(len(train_label))]   #size_batch*len_sentence_without_-100\n",
    "  label_clean = [train_label[k,(train_label[k]!=-100)&(train_label[k]!=8)] for k in range(len(train_label))]   #size_batch     \n",
    "\n",
    "  ###calculate prediction and accuracy\n",
    "  prediction=[logits_clean[i].argmax(dim=1) for i in range(len(logits_clean))]\n",
    "\n",
    "  for i in range(len(logits_clean)):\n",
    "    if prediction[i].shape[0]!=0:\n",
    "      acc_batch.append((prediction[i]==label_clean[i]).float().mean())\n",
    "      nb_valid_label+=1\n",
    "  acc=torch.tensor(acc_batch).sum()\n",
    "  return acc,nb_valid_label\n"
   ],
   "metadata": {
    "id": "h5LlEixbrqcC"
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4EQcYsUJucN"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "t-C8MWiGJucN"
   },
   "outputs": [],
   "source": [
    "def train_loop(model, df_train, df_val,optimizer,EPOCHS,accumulation_steps,permit_decrease,Unkown_label):\n",
    "\n",
    "    #Dataloading\n",
    "    train_dataset = DataSequence(df_train,labels_to_ids,label_all_tokens) #output a tuple: (dict of input, list of label)\n",
    "    val_dataset = DataSequence(df_val,labels_to_ids,label_all_tokens)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)   #all in one batch\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=2)\n",
    "    \n",
    "    \n",
    "    #GPU / CPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    #device='cpu'\n",
    "    \n",
    "    #Presetiing\n",
    "    model=model.half().to(device)\n",
    "    best_val_acc=0\n",
    "    nb_decreasing_acc=0\n",
    "    \n",
    "    list_loss_train=[]\n",
    "    list_acc_val=[]\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        total_nb_valide_example_train=0\n",
    "        total_nb_valide_example_val=0\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        step=0\n",
    "\n",
    "        ######Training######\n",
    "        model.train()\n",
    "\n",
    "        for train_data, train_label in tqdm(train_dataloader): \n",
    "            step=step+1\n",
    "            train_label = train_label.to(device)   #get label\n",
    "            mask = train_data['attention_mask'].to(device)   #get attention mask\n",
    "            input_id = train_data['input_ids'].to(device)   \n",
    "            \n",
    "            #####Sqe useless dim\n",
    "            input_id_sqe=torch.squeeze(input_id)\n",
    "            mask_sqe=torch.squeeze(mask)\n",
    "            \n",
    "            #####Forward\n",
    "            loss, logits = model(input_id_sqe, mask_sqe, train_label)   #3 input to model(see class BertModel for details)\n",
    "                                                                #loss is obvious the loss function \n",
    "                                                                #logit is the \"raw output\" of the model(quite nornal in classification model)\n",
    "                                                                #0<logits<1, for multi-classification task, it offen pass through a softmax, \n",
    "                                                                #then we get probability of  each class\n",
    "            \n",
    "            \n",
    "            ####For calculate acc, should clean off meanless data: -100\n",
    "            acc,nb_valid_example=acc_calculation(logits,train_label)\n",
    "            total_nb_valide_example_train+=nb_valid_example\n",
    "\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            #####With accu to save GPU\n",
    "            \n",
    "            loss = loss/accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if((step+1)%accumulation_steps)==0:\n",
    "              optimizer.step()        # ÂèçÂêë‰º†Êí≠ÔºåÊõ¥Êñ∞ÁΩëÁªúÂèÇÊï∞\n",
    "              optimizer.zero_grad()   # Ê∏ÖÁ©∫Ê¢ØÂ∫¶\n",
    "        if(step==1):\n",
    "          break\n",
    "            \n",
    "\n",
    "        ########evaluation######\n",
    "\n",
    "        model.eval()\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        for val_data, val_label in val_dataloader:\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_data['attention_mask'].to(device)\n",
    "            input_id = val_data['input_ids'].to(device)\n",
    "            \n",
    "            ##Forward\n",
    "            loss, logits = model(input_id, mask, val_label)\n",
    "\n",
    "            acc,nb_valid_example=acc_calculation(logits,val_label)\n",
    "            total_nb_valide_example_val+=nb_valid_example\n",
    "\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += loss.item()\n",
    "\n",
    "        #####Early stop#####\n",
    "        if best_val_acc<(total_acc_val / step):\n",
    "          best_val_acc=total_acc_val / step\n",
    "        else:\n",
    "          nb_decreasing_acc+=1\n",
    "        if nb_decreasing_acc==permit_decrease:\n",
    "          print('\\n\\n Overall fitting avoiding! ')\n",
    "          break\n",
    "\n",
    "        \n",
    "        val_accuracy = total_acc_val / len(df_val)\n",
    "        val_loss = total_loss_val / len(df_val)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / total_nb_valide_example_train: .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / total_nb_valide_example_val: .3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training Test"
   ],
   "metadata": {
    "id": "ja67f609MuJr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#  val_dataset = DataSequence(df_val,labels_to_ids,label_all_tokens)\n",
    "#  val_dataloader = DataLoader(val_dataset, batch_size=1)\n",
    "#  device='cuda'\n",
    "#  for val_data, val_label in val_dataloader:\n",
    "   \n",
    "\n",
    "#     val_label = val_label[0].to(device)\n",
    "#     mask = val_data['attention_mask'][0].to(device)\n",
    "\n",
    "#     input_id = val_data['input_ids'][0].to(device)\n",
    "    \n",
    "#     print(val_label)\n",
    "#     loss, logits = model(input_id, mask, val_label)\n",
    "\n",
    "#     logits_clean = logits[0][val_label != -100]\n",
    "#     label_clean = val_label[val_label != -100]\n",
    "\n",
    "#     predictions = logits_clean.argmax(dim=1)          \n",
    "\n",
    "#     acc = (predictions == label_clean).float().mean()\n",
    "\n"
   ],
   "metadata": {
    "id": "1JHdsZBslsvp"
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "jF3qHH8sJucO",
    "outputId": "bae01749-a814-43ef-8cc2-3f9d37ff39c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n\\nLEARNING_RATE = 1e-2\\nEPOCHS = 5\\ndevice = 'cpu'\\nmodel = BertModel()\\noptimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\\n\\n\\ntrain_dataset = DataSequence(df_train,labels_to_ids,label_all_tokens) #output a tuple: (dict of input, list of label)\\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)   #all in one batch\\n\\nii=0\\n\\n\\n\\nfor train_data, train_label in tqdm(train_dataloader):\\n    ii=ii+1\\n\\n    \\n    train_label = train_label.to(device)   #get label\\n    mask = train_data['attention_mask'].to(device)   #get attention mask\\n    input_id = train_data['input_ids'].to(device)   \\n    input_id_sqe=torch.squeeze(input_id)\\n    mask_sqe=torch.squeeze(mask)\\n\\n    optimizer.zero_grad()\\n\\n    loss, logits = model(input_id_sqe, mask_sqe, train_label)\\n    logits_clean = [logits[i,train_label[i] != -100] for i in range(len(train_label))]   #size_batch*len_sentence_without_-100\\n    label_clean = [train_label[k,train_label[k] != -100] for k in range(len(train_label))]   #size_batch\\n    \\n    prediction=[logits_clean[i].argmax(dim=1) for i in range(len(logits_clean))]\\n    acc_batch=[(prediction[i]==label_clean[i]).float().mean() for i in range(len(logits_clean))]\\n    acc=torch.tensor(acc_batch).mean()\\n\\n    print(acc)\\n    if ii==5:\\n        break\\n\""
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########testtttttt   \n",
    "########Squeezeee!!! before input in model(bert can't accept matrics more than 3 dim)\n",
    "######DO test on a subcell!!\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "LEARNING_RATE = 1e-2\n",
    "EPOCHS = 5\n",
    "device = 'cpu'\n",
    "model = BertModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "train_dataset = DataSequence(df_train,labels_to_ids,label_all_tokens) #output a tuple: (dict of input, list of label)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)   #all in one batch\n",
    "\n",
    "ii=0\n",
    "\n",
    "\n",
    "\n",
    "for train_data, train_label in tqdm(train_dataloader):\n",
    "    ii=ii+1\n",
    "\n",
    "    \n",
    "    train_label = train_label.to(device)   #get label\n",
    "    mask = train_data['attention_mask'].to(device)   #get attention mask\n",
    "    input_id = train_data['input_ids'].to(device)   \n",
    "    input_id_sqe=torch.squeeze(input_id)\n",
    "    mask_sqe=torch.squeeze(mask)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss, logits = model(input_id_sqe, mask_sqe, train_label)\n",
    "    logits_clean = [logits[i,train_label[i] != -100] for i in range(len(train_label))]   #size_batch*len_sentence_without_-100\n",
    "    label_clean = [train_label[k,train_label[k] != -100] for k in range(len(train_label))]   #size_batch\n",
    "    \n",
    "    prediction=[logits_clean[i].argmax(dim=1) for i in range(len(logits_clean))]\n",
    "    acc_batch=[(prediction[i]==label_clean[i]).float().mean() for i in range(len(logits_clean))]\n",
    "    acc=torch.tensor(acc_batch).mean()\n",
    "\n",
    "    print(acc)\n",
    "    if ii==5:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "9aa5iu1hJucP"
   },
   "outputs": [],
   "source": [
    "# mask = train_data['attention_mask'].to(device)   #get attention mask\n",
    "# input_id = train_data['input_ids'].to(device)   \n",
    "# input_id_sqe=torch.squeeze(input_id)\n",
    "# mask_sqe=torch.squeeze(mask)\n",
    "\n",
    "# loss, logits = model(input_id, mask, train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ejbRQFvaJucP"
   },
   "outputs": [],
   "source": [
    "# prediction=[logits_clean[i].argmax(dim=1) for i in range(len(logits_clean))]\n",
    "\n",
    "\n",
    "# a=[(prediction[i]==label_clean[i]).float().mean() for i in range(len(logits_clean))]\n",
    "# acc=torch.tensor(a).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Start training"
   ],
   "metadata": {
    "id": "dtUJPkukM6qF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L1ViaYuIJucP",
    "outputId": "d0bc80ed-3d2f-4301-c931-1db862ff4ce5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 2.00 GiB total capacity; 1.21 GiB already allocated; 0 bytes free; 1.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-50-fca504072c71>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;31m#############Start training###########\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mKeep_training\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 43\u001B[1;33m   \u001B[0mtrain_loop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_val\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0maccumulation_steps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maccumulation_steps\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpermit_decrease\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpermit_decrease\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mUnkown_label\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mUnkown_label\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-44-b0c427db6cce>\u001B[0m in \u001B[0;36mtrain_loop\u001B[1;34m(model, df_train, df_val, optimizer, EPOCHS, accumulation_steps, permit_decrease, Unkown_label)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[1;31m#Presetiing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m     \u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhalf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m     \u001B[0mbest_val_acc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[0mnb_decreasing_acc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mto\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    897\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_floating_point\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_complex\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_blocking\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    898\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 899\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconvert\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    900\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    901\u001B[0m     def register_backward_hook(\n",
      "\u001B[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    568\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    569\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 570\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    571\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    572\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    568\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    569\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 570\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    571\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    572\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    568\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    569\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 570\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    571\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    572\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    568\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    569\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 570\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    571\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    572\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    591\u001B[0m             \u001B[1;31m# `with torch.no_grad():`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    592\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 593\u001B[1;33m                 \u001B[0mparam_applied\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    594\u001B[0m             \u001B[0mshould_use_set_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparam_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    595\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mshould_use_set_data\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mconvert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    895\u001B[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001B[0;32m    896\u001B[0m                             non_blocking, memory_format=convert_to_format)\n\u001B[1;32m--> 897\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_floating_point\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_complex\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_blocking\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    898\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    899\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconvert\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 2.00 GiB total capacity; 1.21 GiB already allocated; 0 bytes free; 1.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#####Model path\n",
    "if Colab:\n",
    "  model_save_name = 'Bert_chinese_formal.pt'\n",
    "  Path=F\"/content/drive/MyDrive/Colab Notebooks/Output/{model_save_name}\"\n",
    "else:\n",
    "  Path=f'./Bert_chinese.pt'\n",
    "\n",
    "####################################\n",
    "#####Setting before training######\n",
    "####################################\n",
    "Start_new_training=True\n",
    "Load_from_driver=False\n",
    "Keep_training=True\n",
    "\n",
    "#####Model loading\n",
    "if Start_new_training|Load_from_driver:\n",
    "  torch.cuda.empty_cache()\n",
    "  model=BertModel()\n",
    "\n",
    "if Load_from_driver:\n",
    "  torch.cuda.empty_cache()\n",
    "  model.load_state_dict(torch.load(Path))\n",
    "\n",
    "\n",
    "##########################################\n",
    "#################Dataloading##############\n",
    "##########################################\n",
    "df = df[0:1000]    #we pick only 1000 example\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), [int(.8 * len(df)), int(.9 * len(df))])   \n",
    "\n",
    "\n",
    "\n",
    "##############Parameter setting##########\n",
    "LEARNING_RATE = 0.5e-2   \n",
    "EPOCHS = 20\n",
    "accumulation_steps=8    #with accumutlation_step bigger than 1, we can save the usage of GPU storage\n",
    "LEARNING_RATE=LEARNING_RATE*accumulation_steps  #Lr should be increased, or else the training will be too slow\n",
    "permit_decrease=3\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "Unkown_label=8\n",
    "#############Start training###########\n",
    "if Keep_training:\n",
    "  train_loop(model, df_train, df_val,optimizer=optimizer,EPOCHS=EPOCHS,accumulation_steps=accumulation_steps,permit_decrease=permit_decrease,Unkown_label=Unkown_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate"
   ],
   "metadata": {
    "id": "S8s25EGGP4iA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, df_test):\n",
    "    test_dataset = DataSequence(df_test,labels_to_ids,label_all_tokens)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0.0\n",
    "\n",
    "    for test_data, test_label in test_dataloader:\n",
    "\n",
    "        test_label = test_label[0].to(device)\n",
    "        mask = test_data['attention_mask'][0].to(device)\n",
    "        input_id = test_data['input_ids'][0].to(device)\n",
    "          \n",
    "        loss, logits = model(input_id, mask, test_label.long())\n",
    "\n",
    "        logits_clean = logits[0][test_label != -100]\n",
    "        label_clean = test_label[test_label != -100]\n",
    "\n",
    "        predictions = logits_clean.argmax(dim=1)\n",
    "              \n",
    "        acc = (predictions == label_clean).float().mean()\n",
    "        total_acc_test += acc\n",
    "\n",
    "    val_accuracy = total_acc_test / len(df_test)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
    "\n",
    "\n",
    "evaluate(model, df_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASarYTrOZvi0",
    "outputId": "5737ef41-9a1e-4eb2-a2ca-1396b97777c1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Accuracy:  0.969\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def align_word_ids(texts):\n",
    "  \n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1 if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "\n",
    "def evaluate_one_text(model, sentence):\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    mask = text['attention_mask'][0].unsqueeze(0).to(device)\n",
    "\n",
    "    input_id = text['input_ids'][0].unsqueeze(0).to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(input_id, mask, None)\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "\n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
    "    print(sentence)\n",
    "    print(prediction_label)\n",
    "            \n",
    "evaluate_one_text(model, 'ÊàëÂ∑≤ÁªèÂà∞Ê≤àÈò≥‰∫ÜÂó∑ÔºåÊåáÂÆöÊ≤°Êúâ‰Ω†Â•ΩÊûúÊ±ÅÂêÉ')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2RcPF91P7FS",
    "outputId": "6ff4203e-77da-46d4-c208-85c315873add"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ÊàëÂ∑≤ÁªèÂà∞Ê≤àÈò≥‰∫ÜÂó∑ÔºåÊåáÂÆöÊ≤°Êúâ‰Ω†Â•ΩÊûúÊ±ÅÂêÉ\n",
      "['O', 'O', 'O', 'O', 'B_LOC', 'I_LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ER8Tiv6ZP95w"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d07b84835cd9e344cfc6b36587121331cd748b79cb07c96e023b0204f3468bf8"
   }
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "ja67f609MuJr"
   ]
  },
  "gpuClass": "standard",
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e065e60755f6426186019993cd83f68b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ad8c77c860f6481581e370f21d6b8621",
       "IPY_MODEL_3a7e5e1b461d4356adddbc0cc2230f12",
       "IPY_MODEL_c71096b610684c5ca9a688abac7b93ab"
      ],
      "layout": "IPY_MODEL_cd28f3f9c1fe4ae6b4422f908ca832dd"
     }
    },
    "ad8c77c860f6481581e370f21d6b8621": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd8269fd57e6488fa779ff9affb4fd79",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_20446dd83deb41c9980cb04198923221",
      "value": "Downloading tokenizer_config.json: 100%"
     }
    },
    "3a7e5e1b461d4356adddbc0cc2230f12": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14e509b410ef4dc29b009bcd3d97ec68",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51d8f1fee1db47d7875263cda6d167dc",
      "value": 29
     }
    },
    "c71096b610684c5ca9a688abac7b93ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7af4cead3e24e329ae30cab1e63bccf",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c5ba7995840c4276be6e60cc7a5eab68",
      "value": " 29.0/29.0 [00:00&lt;00:00, 787B/s]"
     }
    },
    "cd28f3f9c1fe4ae6b4422f908ca832dd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd8269fd57e6488fa779ff9affb4fd79": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20446dd83deb41c9980cb04198923221": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14e509b410ef4dc29b009bcd3d97ec68": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51d8f1fee1db47d7875263cda6d167dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e7af4cead3e24e329ae30cab1e63bccf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5ba7995840c4276be6e60cc7a5eab68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6917aa5edce467b8588326817f31456": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0d498efdd7d41388d584731766857b5",
       "IPY_MODEL_efbc5f11c21a43f6ab34c98ef09bcd1c",
       "IPY_MODEL_bc71afe9bb58448f82671976842e11ab"
      ],
      "layout": "IPY_MODEL_6164fce756d64776adc4d72dd4a639d1"
     }
    },
    "b0d498efdd7d41388d584731766857b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_531846296ec74feda807d6cf16d1b39d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_961234984ce74cd2a735a9fb95da4828",
      "value": "Downloading vocab.txt: 100%"
     }
    },
    "efbc5f11c21a43f6ab34c98ef09bcd1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_097e5374ef9b44f2bd5e165156df56b6",
      "max": 109540,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47e6774978c843e9986e538a68721724",
      "value": 109540
     }
    },
    "bc71afe9bb58448f82671976842e11ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd97bc185757489db76afafa1b21a132",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3f4f930c8f5a45be9f243b476e27ac2d",
      "value": " 107k/107k [00:00&lt;00:00, 333kB/s]"
     }
    },
    "6164fce756d64776adc4d72dd4a639d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "531846296ec74feda807d6cf16d1b39d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "961234984ce74cd2a735a9fb95da4828": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "097e5374ef9b44f2bd5e165156df56b6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47e6774978c843e9986e538a68721724": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cd97bc185757489db76afafa1b21a132": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f4f930c8f5a45be9f243b476e27ac2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "faa046bbf5ce497bbf6f3e7ee8d1a828": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d50b987d9e84e4c97fb11e288814a14",
       "IPY_MODEL_24d1d6b9a112406f904c35f693dfbfd8",
       "IPY_MODEL_242a2025e4744453b4d8c209eb0d478e"
      ],
      "layout": "IPY_MODEL_c946015d74c748fd9e24ba6e6a285007"
     }
    },
    "9d50b987d9e84e4c97fb11e288814a14": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6990ee6a22494ec282cc424ba4757a52",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cdda716bac6c4de092dfcea20163188a",
      "value": "Downloading tokenizer.json: 100%"
     }
    },
    "24d1d6b9a112406f904c35f693dfbfd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d7d54bba2cd48f0bca1e479d8f0367b",
      "max": 268943,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cacee7a3bb9343b7bf6c5b069b635ed9",
      "value": 268943
     }
    },
    "242a2025e4744453b4d8c209eb0d478e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eee57270de964cbaadceb07a8515eb93",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_27012be861e1432ab4b237e48a0c25f4",
      "value": " 263k/263k [00:00&lt;00:00, 862kB/s]"
     }
    },
    "c946015d74c748fd9e24ba6e6a285007": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6990ee6a22494ec282cc424ba4757a52": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdda716bac6c4de092dfcea20163188a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d7d54bba2cd48f0bca1e479d8f0367b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cacee7a3bb9343b7bf6c5b069b635ed9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eee57270de964cbaadceb07a8515eb93": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27012be861e1432ab4b237e48a0c25f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d95c054583946499f2f3e500b72d927": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4dd3e3b7ab324cc8ababdd8a447263f8",
       "IPY_MODEL_39867111d6ba404e8db39f783e884543",
       "IPY_MODEL_ca7bab79d925449fbe520e34871192cf"
      ],
      "layout": "IPY_MODEL_c75453ebeca8488788009c2e9c722553"
     }
    },
    "4dd3e3b7ab324cc8ababdd8a447263f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d6637412ca641d0a605cf348ba0cc39",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9db41e0582514932b56f64ecf337d2ee",
      "value": "Downloading config.json: 100%"
     }
    },
    "39867111d6ba404e8db39f783e884543": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25848c0cd7654e9f836c16193307807a",
      "max": 624,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_db599e29c05449d6af1f9137de95f940",
      "value": 624
     }
    },
    "ca7bab79d925449fbe520e34871192cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56928713eec84d3e823110ebac8c87cf",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e23a833aad07459bbd42247f84f07638",
      "value": " 624/624 [00:00&lt;00:00, 20.3kB/s]"
     }
    },
    "c75453ebeca8488788009c2e9c722553": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d6637412ca641d0a605cf348ba0cc39": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9db41e0582514932b56f64ecf337d2ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25848c0cd7654e9f836c16193307807a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db599e29c05449d6af1f9137de95f940": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "56928713eec84d3e823110ebac8c87cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e23a833aad07459bbd42247f84f07638": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
