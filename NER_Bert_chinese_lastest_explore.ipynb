{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTmNj95EJub3"
   },
   "source": [
    "# Named Entity Recognition with BERT in PyTorch\n",
    "Based: https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a  \n",
    "library: Hugging face's \"Transformer\"   \n",
    "Êï∞ÊçÆÂ∫ìÂ≠òÂú®ËÑèÊï∞ÊçÆÔºÅlabelÂíåËØçÈïøÂ∫¶‰∏ç‰∏ÄËá¥  \n",
    "We use \"BertFortokenClassification\" instead of \"BertForSequenceClassification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "f_akTVv8EAkx"
   },
   "outputs": [],
   "source": [
    "# Mode chosing\n",
    "Colab=False\n",
    "Remote_server=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3Ycr6F5J76T",
    "outputId": "9df99e1c-b59b-413c-c5da-4fec492f664d"
   },
   "outputs": [],
   "source": [
    "#######For google\n",
    "if Colab:\n",
    "    !pip install transformers\n",
    "if Remote_server:\n",
    "    Work_path='/workspace/Bert_Chinese/'\n",
    "else:\n",
    "    Work_path='./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uTa_0utiJub8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertForTokenClassification\n",
    "from torch.utils.data import DataLoader    \n",
    "#if we use import torch.utils.data.DataLoader as Dataloader, here dataloader is a module\n",
    "#but here, from torch,utils.data import dataloader is now a function\n",
    "import torch.optim as optim   #Here optim is still a module, we always use optim.SGD to create a function SGD\n",
    "from tqdm import tqdm   #same reason, if we \"import tqdm\" directly, will be error: module is not callable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchcrf import CRF\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_XQJrOzKfgp",
    "outputId": "c227ba49-906e-4eb9-cdcc-7971390c8cac"
   },
   "outputs": [],
   "source": [
    "########Load from gdrive\n",
    "if Colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Input/People_dalily_10000_examples_adapted.csv\")\n",
    "\n",
    "\n",
    "####load(local) \n",
    "else:\n",
    "    #df=pd.read_csv('/workspace/Bert_Chinese/People_dalily_10000_examples_adapted.csv')\n",
    "    df=pd.read_csv(Work_path+'People_dalily_10000_examples_adapted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "9slZGLglJub-",
    "outputId": "f433d064-ad42-4d69-e9ab-08dfa8de0d1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  ‰∫∫ Ê∞ë ÁΩë 1 Êúà 1 Êó• ËÆØ ÊçÆ „Ää Á∫Ω Á∫¶ Êó∂ Êä• „Äã Êä• ÈÅì , Áæé ÂõΩ Âçé Â∞î Ë°ó ...   \n1  „Ää Á∫Ω Á∫¶ Êó∂ Êä• „Äã Êä• ÈÅì ËØ¥ , Ê†á ÊôÆ 5 0 0 Êåá Êï∞ ‰ªä Âπ¥ ‰∏ä Âçá 2 9 ...   \n2  Â∞± 1 2 Êúà 3 1 Êó• Êù• ËØ¥ , Áî± ‰∫é Â∞± ‰∏ö Ââç ÊôØ Áúã Â•Ω Âíå Áªè Êµé Â¢û Èïø ...   \n3  Âè¶ ÊçÆ „Ää Âçé Â∞î Ë°ó Êó• Êä• „Äã Êä• ÈÅì , 2 0 1 3 Âπ¥ ÊòØ 1 9 9 5 Âπ¥ ...   \n4  ‰∫∫ Ê∞ë ÁΩë Âπ≥ Â£§ 1 Êúà 1 Êó• Áîµ ( ËÆ∞ ËÄÖ Áéã Ëéâ „ÄÅ Á®ã Áª¥ ‰∏π ) Êúù È≤ú ÊúÄ ...   \n\n                                              labels  \n0  O O O B_T I_T I_T I_T O O O B_LOC I_LOC O O O ...  \n1  O B_LOC I_LOC O O O O O O O O O O O O O O B_T ...  \n2  O B_T I_T I_T I_T I_T I_T O O O O O O O O O O ...  \n3  O O O B_LOC I_LOC I_LOC O O O O O O B_T I_T I_...  \n4  O O O B_LOC I_LOC I_T I_T I_T I_T O O O O B_PE...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>‰∫∫ Ê∞ë ÁΩë 1 Êúà 1 Êó• ËÆØ ÊçÆ „Ää Á∫Ω Á∫¶ Êó∂ Êä• „Äã Êä• ÈÅì , Áæé ÂõΩ Âçé Â∞î Ë°ó ...</td>\n      <td>O O O B_T I_T I_T I_T O O O B_LOC I_LOC O O O ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>„Ää Á∫Ω Á∫¶ Êó∂ Êä• „Äã Êä• ÈÅì ËØ¥ , Ê†á ÊôÆ 5 0 0 Êåá Êï∞ ‰ªä Âπ¥ ‰∏ä Âçá 2 9 ...</td>\n      <td>O B_LOC I_LOC O O O O O O O O O O O O O O B_T ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Â∞± 1 2 Êúà 3 1 Êó• Êù• ËØ¥ , Áî± ‰∫é Â∞± ‰∏ö Ââç ÊôØ Áúã Â•Ω Âíå Áªè Êµé Â¢û Èïø ...</td>\n      <td>O B_T I_T I_T I_T I_T I_T O O O O O O O O O O ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Âè¶ ÊçÆ „Ää Âçé Â∞î Ë°ó Êó• Êä• „Äã Êä• ÈÅì , 2 0 1 3 Âπ¥ ÊòØ 1 9 9 5 Âπ¥ ...</td>\n      <td>O O O B_LOC I_LOC I_LOC O O O O O O B_T I_T I_...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>‰∫∫ Ê∞ë ÁΩë Âπ≥ Â£§ 1 Êúà 1 Êó• Áîµ ( ËÆ∞ ËÄÖ Áéã Ëéâ „ÄÅ Á®ã Áª¥ ‰∏π ) Êúù È≤ú ÊúÄ ...</td>\n      <td>O O O B_LOC I_LOC I_T I_T I_T I_T O O O O B_PE...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df=pd.read_csv('ner.csv')   #Not completed, like \"Demonstrators\" not regard as ent\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrGeBoY6JucA"
   },
   "source": [
    "## Give ids to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bK_pZSJmJucB",
    "outputId": "c9ac5345-bac3-4bfa-c483-ffc680d9e619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B_LOC', 1: 'B_ORG', 2: 'B_PER', 3: 'B_T', 4: 'I_LOC', 5: 'I_ORG', 6: 'I_PER', 7: 'I_T', 8: 'O'}\n"
     ]
    }
   ],
   "source": [
    "# Split labels based on whitespace and turn them into a list\n",
    "labels = [i.split() for i in df['labels'].values.tolist()]\n",
    "\n",
    "# Check how many labels are there in the dataset\n",
    "unique_labels = set()\n",
    "\n",
    "for lb in labels:\n",
    "  [unique_labels.add(i) for i in lb if i not in unique_labels]   #here .add is fun without return value, thus if we set a=[...], \n",
    "                                                                  # a will =[]\n",
    "\n",
    "# Map each label into its id representation and vice versa\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "Max_len=512\n",
    "print(ids_to_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf0A1ZezJucC"
   },
   "source": [
    "## Tokenize(Output directly become tensor)\n",
    "Bert tokenizer can transform a sentence in nl to a list of number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "f816a3771dca4f4ba559ac3ad41213bb",
      "a0d88595282e4d3c9cbb558f39826fa1",
      "2fd2ca56be0c40c89a18406f4ddfcee0",
      "1895bcecaad248f1a3629ca628c0113d",
      "e146fc0fdad84efbbb060632723d391c",
      "bf106b03f01f43378a87170bf130cb9b",
      "349e61635cc74c3880e1dcdc424fbefa",
      "f00e2a50058944478fda51f7fd5cfe5b",
      "cd468a3294e94e248920887ec1fd1247",
      "f80873f4dc5245f1acbd5dcd06e682d2",
      "5e94c4476ad04f4680e1a76eaa563cca",
      "554e0596bc924337beb2e7903f1932f4",
      "d5eab144ca1346fd89d614753ad8fad0",
      "f2e1dec4abfe4076b32567bdd67ec738",
      "18619a87d26f44769572d9b86fe27b2f",
      "3e9c6c4b3f5849069716235ad5a59a6c",
      "927a24da570342b093add93c64406361",
      "f84a480e089d4364bc9c2860a5e76269",
      "39b2ec0232cc4c8d9f94edbd6f6f5824",
      "21a253b6aba54884bc6c461e2aca5638",
      "b1b34f2edb3a4b1db14f0771065d7142",
      "1a59fe7300b2425e8f8022516e2596ff",
      "c093c6bfe8ab4b69afdb8e63a7a72819",
      "f3f82e10e0ea4231b254d68d190325f4",
      "2f738a04f2b64ad3b3769c5e2bfa6947",
      "844fe36ff8b44ff197623646fede713f",
      "f0c00fa682254c1fbcb0f9db06b8d5b2",
      "fb8a3663a8cc40d288ecc0352baae919",
      "b3c99eb84abb410d9888f87631803b22",
      "ec495c84fa9a45e5ac826c004d41ef81",
      "0b30e1f9dd8145dc896b42cf5f6e75db",
      "c614b67cadb9465492de63927172911f",
      "b8a66518d8dc4853a607a968e51d2cea",
      "f3c4a4db469d436c9ef388d018c8fd33",
      "2f85a6c8c3e94cc2ac9a7ceb2117c84e",
      "26440bb6e9ec49949fb645db9ef4ea97",
      "2e8f932a01244043924c9270c7723e88",
      "f048a8857a654458af82e4f53ada6ce5",
      "dd8efd9f3b844144b3d20bacb0ce1cee",
      "f6201b0f0b00482bb50c6034380e1f98",
      "62fa584e4b8f4699bea1479002ff9f81",
      "701f66158a644085a7648fd5279006d2",
      "b19e14b78e844adc9068ea51cef7faf9",
      "1d6de102451b4d95962167f504afcd08"
     ]
    },
    "id": "51TrjL5aJucC",
    "outputId": "8d54d543-1189-476b-e376-2c8b73e36c82"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at how can we preprocess the text - Take first example\n",
    "text = df['text'].values.tolist()\n",
    "example = text[36]   #get one sentence (THE 36th sentence)\n",
    "\n",
    "#tokenize by bert\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "text_tokenized = tokenizer(example, padding='max_length', max_length=Max_len, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "#3 layer can be found: input_ids, token_type_ids,atention_mask\n",
    "#print(text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L9Vz6v7JucD"
   },
   "source": [
    "### decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TXFVfD1rJucE"
   },
   "outputs": [],
   "source": [
    "#print(tokenizer.decode(text_tokenized.input_ids[0][0:50]))    #0 is the first sentence,since here we have only one sentence to tokenize\n",
    "#we can't decode more than 1 sentence with .decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcNmzzKiJucF"
   },
   "source": [
    "### \"Convert ids to token\" will find Bert's\"subword\" problem\n",
    "The BERT tokenizer uses the so-called word-piece tokenizer under the hood, which is a sub-word tokenizer. This means that BERT tokenizer will likely to **split one word into one or more meaningful sub-words**.\n",
    "*Which make label provided by database can't match tokens*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oe5YyieKJucH",
    "outputId": "30be1e7b-ed79-4c73-a0ae-0815282247c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Êñ∞', 'Âπ¥', 'Ââç', 'Â§ï', ',', 'ÂõΩ', 'ÂÆ∂', '‰∏ª', 'Â∏≠', '‰π†', 'Ëøë', 'Âπ≥', 'ÈÄö', 'Ëøá', '‰∏≠', 'ÂõΩ', 'ÂõΩ', 'ÈôÖ', 'Âπø', 'Êí≠', 'Áîµ', 'Âè∞', '„ÄÅ', '‰∏≠', 'Â§Æ', '‰∫∫', 'Ê∞ë', 'Âπø', 'Êí≠', 'Áîµ', 'Âè∞', '„ÄÅ', '‰∏≠', 'Â§Æ', 'Áîµ', 'ËßÜ', 'Âè∞', ',', 'Âèë', 'Ë°®', '‰∫Ü', '2', '0', '1', '4', 'Âπ¥', 'Êñ∞', 'Âπ¥', 'Ë¥∫']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n"
     ]
    }
   ],
   "source": [
    "######Check output of word_ids, we will find it pretty smart:\n",
    "######First token [CLS] is marked as None, which avoid the movement of whole sentence\n",
    "######All words are marked with their real \"index\" in \"labels list\"(0st word is Prime, and 3rd word is G+ei+r)\n",
    "word_ids = text_tokenized.word_ids()\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0])[0:50])\n",
    "print(word_ids[0:50])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ1avpX_JucH"
   },
   "source": [
    "### Two method can be applied to solve this problem:\n",
    "These word_ids will be very useful to adjust the length of the label by applying either of these two methods:  \n",
    "\n",
    "1, We only provide a label to the first sub-word of each splitted token. The continuation of the sub-word then will simply have ‚Äò-100‚Äô as a label. All tokens that don‚Äôt have word_ids will also be labeled with ‚Äò-100‚Äô.  \n",
    "2, We provide the same label among all of the sub-words that belong to the same token. All tokens that don‚Äôt have word_ids will be labeled with ‚Äò-100‚Äô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hlsdh7_cJucI"
   },
   "outputs": [],
   "source": [
    "def align_label_example(tokenized_input, labels,labels_to_ids,label_all_tokens):\n",
    "    '''\n",
    "    output: labels_ids\n",
    "    \n",
    "    '''\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    #print('word')\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "            \n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])  #As we mention, word_idx is exactly \n",
    "            except:\n",
    "                label_ids.append(-100)   #Ê≠§Â§ÑÊòØÈÅøÂÖçËÑèÊï∞ÊçÆÁöÑÂΩ±ÂìçÔºàÂç≥labelÁöÑÈïøÂ∫¶ÂíåÂÆûÈôÖÁöÑÂè•Â≠êÈïøÂ∫¶‰∏çÂêåÔºâ\n",
    "    \n",
    "        else:\n",
    "            # print('label_id', label_ids)\n",
    "            # print('word_ids ',word_ids[0:50])\n",
    "            # #print('label[word]', labels[word_idx])\n",
    "            # print('label list ', len(labels))\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "    \n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-fUGFacJucJ"
   },
   "source": [
    "## New lables for bert to train\n",
    "Since original labels can't match with its token list, we create a new lables list to fit it  \n",
    "what is more, we present labels in ids form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryHR4S01JucJ",
    "outputId": "b609d770-5734-4ae8-c58f-f8fc80324e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 3, 7, 8, 8, 8, 8, 8, 8, 8, 2, 6, 6, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "['[CLS]', 'Êñ∞', 'Âπ¥', 'Ââç', 'Â§ï', ',', 'ÂõΩ', 'ÂÆ∂', '‰∏ª', 'Â∏≠', '‰π†', 'Ëøë', 'Âπ≥', 'ÈÄö', 'Ëøá', '‰∏≠', 'ÂõΩ', 'ÂõΩ', 'ÈôÖ', 'Âπø', 'Êí≠', 'Áîµ', 'Âè∞', '„ÄÅ', '‰∏≠']\n"
     ]
    }
   ],
   "source": [
    "label = labels[36]\n",
    "\n",
    "#If we set label_all_tokens to True.....\n",
    "label_all_tokens = True\n",
    "\n",
    "new_label = align_label_example(text_tokenized, label,labels_to_ids,label_all_tokens)\n",
    "print(new_label[0:25])   #he\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0])[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZxEVx6GJucK"
   },
   "source": [
    "## Dataset Class(tokenize include)\n",
    "Before we train our BERT model for NER task, we need to create a dataset class to generate and fetch data in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Yj3AkvkmJucK"
   },
   "outputs": [],
   "source": [
    "##### NOt a simple class of dataset, we also realise tokenizer here\n",
    "\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df,labels_to_ids,label_all_tokens):\n",
    "\n",
    "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
    "        txt = df['text'].values.tolist()   #list of sentence\n",
    "        \n",
    "        text_tokenized = [tokenizer(str(i),\n",
    "                               padding='max_length', max_length = Max_len, truncation=True, return_tensors=\"pt\") for i in txt]\n",
    "        \n",
    "        self.texts=text_tokenized\n",
    "         \n",
    "        self.labels = [align_label_example(i,j,labels_to_ids,label_all_tokens) for i,j in zip(text_tokenized, lb)]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_data, batch_labels   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie32NrQLJucL"
   },
   "source": [
    "### Train val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Jl9k250hJucL"
   },
   "outputs": [],
   "source": [
    "df = df[0:1000]    #we pick only 1000 example\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),    \n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])    #split the lisy into 3 parts, with 2 cut\n",
    "                                                                        #one cut at 0.8*len(df)\n",
    "                                                                        #one cut at 0.9*len(df)\n",
    "                                                                        #so train:val:test=8:1:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdalS2vCJucL"
   },
   "source": [
    "#### Test Datasequence (check with class \"Datasequence\"'s return)\n",
    "Find that output of DataSequence is a list with len of nb_sentence,  and each elements is a tuple  \n",
    "\n",
    "elem 1 in truple : Dict with 3 pairs key-value: input_ids, attention mask, token_type_ids   \n",
    "elem 2 in truple: labels's ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "GvlwvPANJucM"
   },
   "outputs": [],
   "source": [
    "# Data_token=DataSequence(df_train[43:100],labels_to_ids,label_all_tokens)\n",
    "# Data_token.__getitem__(20)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdh9J3ilJucM"
   },
   "source": [
    "### Torch's model definition\n",
    "Define a model class in torch's way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "9"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "t1maOjmkJucM"
   },
   "outputs": [],
   "source": [
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,crf=False,Unknow_label=len(unique_labels)-1,device_used='cuda'):       #here we set defaut value for these function,\n",
    "                                            #and we can change them if we want, just by calling: model.crf\n",
    "                                            #Now you know, init means initialize!!, we do it only when we call model=BertModel\n",
    "\n",
    "        super(BertModel, self).__init__()   #for pytorch, this lign is obligatory\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))  #transformer layer\n",
    "        self.crf_layer = CRF(num_tags=len(unique_labels),batch_first=True)    #here since our tensor is batch_size*sqe_len, so here is batch_first\n",
    "        self.crf=crf\n",
    "        self.unknow_label=Unknow_label\n",
    "        self.device_used=device_used\n",
    "        self.max_len=Max_len\n",
    "        \n",
    "    \n",
    "    def forward(self, input_id, mask,label):   #Forward is a special function, we can pass like model(input), and we get the return\n",
    "        if not self.crf:\n",
    "            output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)  \n",
    "                                                                                        #self.bert equal to model\n",
    "                                                                                        #return_dict=false -> return value is a tuple of (loss, logits)\n",
    "        else:\n",
    "            loss,logits=self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False) \n",
    "            \n",
    "            ###adaptation to fit crf\n",
    "            mask_adapted=self.mask_adapted_first_end(mask)\n",
    "            label_adapted=self.filter_label(label)\n",
    "            logits_adapted=self.filter_logits_first(logits)\n",
    "            \n",
    "            ### \n",
    "            loss=self.crf_layer(logits_adapted,label_adapted,mask_adapted)\n",
    "            pre=self.crf_layer.decode(logits_adapted)\n",
    "            output=(loss,pre)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def mask_adapted_first_end(self,mask):\n",
    "        \"\"\"\n",
    "        To adapt the output of bert to input of crf:\n",
    "\n",
    "        Here we delete the last two '1' in the end\n",
    "        1 is for the disappear of first first token \n",
    "        and 1 is for imforming the ML that the last one is not important\n",
    "        \"\"\"\n",
    "        mask_squ=copy.deepcopy(mask)\n",
    "        label_batch_filted=[]\n",
    "        for label_win in mask_squ:\n",
    "            #print('before',(label_win!=0).sum())\n",
    "            len_one=(label_win!=0).sum()\n",
    "            label_win[len_one-2:len_one]=0\n",
    "            #print('after',(label_win!=0).sum())\n",
    "            label_batch_filted.append(label_win)\n",
    "            mask_filted=torch.stack(label_batch_filted)\n",
    "        return mask_filted[:,1:self.max_len].type(torch.bool)\n",
    "    \n",
    "    def filter_label(self,label):\n",
    "        \"\"\"\n",
    "        To adapt the output of bert to input of crf:\n",
    "        1, cut off the first label of each sentence, which is always -100\n",
    "        2, replace all -100 by 8, which will be ignored thanks to mask, (we have to do it or else the program cann't go on)\n",
    "        \"\"\"\n",
    "        Unknow_label=self.unknow_label\n",
    "        device=self.device_used\n",
    "        delete_first_col_label=label[:,1:self.max_len]\n",
    "        label_change_value=torch.where(delete_first_col_label==-100,torch.tensor(Unknow_label).to(device),delete_first_col_label)  \n",
    "        return label_change_value\n",
    "    \n",
    "    def filter_logits_first(self,logits):\n",
    "        \"\"\"\n",
    "        To adapt the output of bert to input of crf:\n",
    "        1, We delete the prediction of first token in each sentence,since we do so in filter_label\n",
    "        \"\"\"\n",
    "        #return torch.index_select(logits,1,(1+torch.IntTensor(range(511))).long().to(self.device_used))\n",
    "        return logits[:,1:self.max_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jysu47ifJucM"
   },
   "source": [
    "### About the warning of BertForTokenClassifica\n",
    "You may occur such warning:   \n",
    "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification:....   \n",
    "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).   \n",
    "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).   \n",
    "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly  initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.   \n",
    "\n",
    "Answer:  \n",
    "https://github.com/huggingface/transformers/issues/5421#issuecomment-652582854  \n",
    "\n",
    "@ohmeow you're loading the **bert-base-cased checkpoint** (which is a checkpoint that was trained using a similar architecture to BertForPreTraining) in a BertForSequenceClassification model.\n",
    "\n",
    "This means that:\n",
    "\n",
    "The layers that BertForPreTraining has, but BertForSequenceClassification does not have will be discarded  \n",
    "The layers that BertForSequenceClassification has but BertForPreTraining does not have will be randomly initialized.  \n",
    "This is expected, and tells you that you won't have good performance with your BertForSequenceClassification model before you   fine-tune it üôÇ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def acc_calculation_crf(pre,train_label,model):\n",
    "    unknow_label=model.unknow_label\n",
    "    device=model.device_used\n",
    "    pre=torch.tensor(pre).to(device)\n",
    "\n",
    "    acc_batch=[]\n",
    "    nb_valid_label=0\n",
    "\n",
    "    train_label=train_label[:,1:model.max_len]\n",
    "    pre_clean = [pre[i,(train_label[i]!=-100)&(train_label[i]!=unknow_label)] for i in range(len(train_label))]\n",
    "    label_clean = [train_label[k,(train_label[k]!=-100)&(train_label[k]!=unknow_label)] for k in range(len(train_label))]\n",
    "\n",
    "    for i in range(len(pre_clean)):\n",
    "        if pre_clean[i].shape[0]!=0:\n",
    "            acc_batch.append((pre_clean[i]==label_clean[i]).float().mean())\n",
    "            nb_valid_label+=1\n",
    "    acc=torch.tensor(acc_batch).sum()\n",
    "    return acc,nb_valid_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "h5LlEixbrqcC"
   },
   "outputs": [],
   "source": [
    "def acc_calculation(logits,train_label,unknow_label):\n",
    "    acc_batch=[]\n",
    "    nb_valid_label=0\n",
    "    logits_clean = [logits[i,(train_label[i]!=-100)&(train_label[i]!=unknow_label)] for i in range(len(train_label))]   #size_batch*len_sentence_without_-100\n",
    "    label_clean = [train_label[k,(train_label[k]!=-100)&(train_label[k]!=unknow_label)] for k in range(len(train_label))]   #size_batch     \n",
    "\n",
    "    ###calculate prediction and accuracy\n",
    "    prediction=[]\n",
    "    for i in range(len(logits_clean)):\n",
    "        if logits_clean[i].shape[0]!=0:\n",
    "            prediction.append(logits_clean[i].argmax(dim=1))\n",
    "        else:\n",
    "            prediction.append(torch.tensor([]))\n",
    "    for i in range(len(logits_clean)):\n",
    "        if prediction[i].shape[0]!=0:\n",
    "            acc_batch.append((prediction[i]==label_clean[i]).float().mean())\n",
    "            nb_valid_label+=1\n",
    "    acc=torch.tensor(acc_batch).sum()\n",
    "    return acc,nb_valid_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4EQcYsUJucN"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "t-C8MWiGJucN"
   },
   "outputs": [],
   "source": [
    "def train_loop(model, df_train, df_val,optimizer,EPOCHS,accumulation_steps,permit_decrease,Unknow_label,writer,writer_epochs):\n",
    "\n",
    "    #Dataloading\n",
    "    train_dataset = DataSequence(df_train,labels_to_ids,label_all_tokens) #output a tuple: (dict of input, list of label)\n",
    "    val_dataset = DataSequence(df_val,labels_to_ids,label_all_tokens)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)   #all in one batch\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "    \n",
    "    \n",
    "    #GPU / CPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    #device='cpu'\n",
    "    \n",
    "    #Presetiing\n",
    "    model=model.to(device)\n",
    "    model.device_used=device\n",
    "    best_val_acc=0\n",
    "    nb_decreasing_acc=0\n",
    "    \n",
    "    list_loss_train=[]\n",
    "    list_acc_val=[]\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        total_nb_valide_example_train=0\n",
    "        total_nb_valide_example_val=0\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        step=0\n",
    "\n",
    "        ######Training######\n",
    "        model.train()\n",
    "\n",
    "        for train_data, train_label in tqdm(train_dataloader): \n",
    "            step=step+1\n",
    "            train_label = train_label.to(device)   #get label\n",
    "            mask = train_data['attention_mask'].to(device)   #get attention mask\n",
    "            input_id = train_data['input_ids'].to(device)   \n",
    "            \n",
    "            #####Sqe useless dim\n",
    "            input_id_sqe=torch.squeeze(input_id)\n",
    "            mask_sqe=torch.squeeze(mask)\n",
    "\n",
    "            #####Forward\n",
    "            if model.crf:\n",
    "                loss,pre=model(input_id_sqe, mask_sqe, train_label)\n",
    "                acc,nb_valid_example=acc_calculation_crf(pre,train_label,model)\n",
    "            else:\n",
    "                loss, logits = model(input_id_sqe, mask_sqe, train_label)   #3 input to model(see class BertModel for details)\n",
    "                                                                    #loss is obvious the loss function\n",
    "                                                                    #logit is the \"raw output\" of the model(quite nornal in classification model)\n",
    "                                                                    #0<logits<1, for multi-classification task, it offen pass through a softmax,\n",
    "                                                                    #then we get probability of  each class\n",
    "                acc,nb_valid_example=acc_calculation(logits,train_label,unknow_label=Unknow_label)\n",
    "            \n",
    "            ####For calculate acc, should clean off meanless data: -100\n",
    "            total_nb_valide_example_train+=nb_valid_example\n",
    "\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            #####With accu to save GPU\n",
    "            \n",
    "            loss = loss/accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if((step+1)%accumulation_steps)==0:\n",
    "              optimizer.step()        # ÂèçÂêë‰º†Êí≠ÔºåÊõ¥Êñ∞ÁΩëÁªúÂèÇÊï∞\n",
    "              optimizer.zero_grad()   # Ê∏ÖÁ©∫Ê¢ØÂ∫¶\n",
    "        if(step==1):\n",
    "          break\n",
    "            \n",
    "\n",
    "        ########evaluation######\n",
    "\n",
    "        model.eval()\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        for val_data, val_label in tqdm(val_dataloader):\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_data['attention_mask'].to(device)\n",
    "            input_id = val_data['input_ids'].to(device)\n",
    "            \n",
    "            #####Sqe useless dim\n",
    "            input_id_sqe=torch.squeeze(input_id)\n",
    "            mask_sqe=torch.squeeze(mask)\n",
    "\n",
    "            ##Forward\n",
    "            loss, logits = model(input_id_sqe, mask_sqe, train_label)\n",
    "\n",
    "            acc,nb_valid_example=acc_calculation(logits,val_label,Unknow_label)\n",
    "            total_nb_valide_example_val+=nb_valid_example\n",
    "\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += loss.item()\n",
    "\n",
    "        #####Early stop#####\n",
    "        if best_val_acc<(total_acc_val / step):\n",
    "            best_val_acc=total_acc_val / step\n",
    "            nb_decreasing_acc=0\n",
    "            torch.save(model.state_dict(), Work_path+'Model_backup/Model_backup.pt')\n",
    "        else:\n",
    "          nb_decreasing_acc+=1\n",
    "        if nb_decreasing_acc==permit_decrease:\n",
    "          print('\\n\\n Overall fitting avoiding! ')\n",
    "          break\n",
    "\n",
    "        \n",
    "        val_accuracy = total_acc_val / len(df_val)\n",
    "        val_loss = total_loss_val / len(df_val)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / total_nb_valide_example_train: .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / total_nb_valide_example_val: .3f}')\n",
    "#         writer.add_scalar(\"loss_train\", total_loss_train / len(df_train), epoch_num+writer_epochs)\n",
    "#         writer.add_scalar(\"acc_train\", total_acc_train / total_nb_valide_example_train, epoch_num+writer_epochs)\n",
    "#         writer.add_scalar(\"loss_val\", total_loss_val / len(df_val), epoch_num+writer_epochs)\n",
    "#         writer.add_scalar(\"acc_val\",total_acc_val / total_nb_valide_example_val,epoch_num+writer_epochs)\n",
    "        \n",
    "    return epoch_num "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtUJPkukM6qF"
   },
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1ViaYuIJucP",
    "outputId": "f8fe7126-5ee2-486f-fe5a-39e79b79149c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 16%|‚ñà‚ñã        | 33/200 [00:25<02:10,  1.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-44-fc35ed44bd1d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0;31m#############Start training###########\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mKeep_training\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m     \u001B[0mepochs_trained\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_loop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf_val\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0maccumulation_steps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maccumulation_steps\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpermit_decrease\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpermit_decrease\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mUnknow_label\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mUnknow_label\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mwriter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mwriter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mwriter_epochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mwriter_epochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m     \u001B[0mwriter_epochs\u001B[0m\u001B[0;34m+=\u001B[0m\u001B[0mepochs_trained\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-43-64eda0b5f51b>\u001B[0m in \u001B[0;36mtrain_loop\u001B[0;34m(model, df_train, df_val, optimizer, EPOCHS, accumulation_steps, permit_decrease, Unknow_label, writer, writer_epochs)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0maccumulation_steps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0;32mif\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m%\u001B[0m\u001B[0maccumulation_steps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[0;32m--> 221\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m    128\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 130\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#####Model path\n",
    "if Colab:\n",
    "  model_save_name = 'Bert_chinese_formal.pt'\n",
    "  Path=F\"/content/drive/MyDrive/Colab Notebooks/Output/{model_save_name}\"\n",
    "else:\n",
    "  Path=Work_path+'Model_backup/Model_backup.pt'\n",
    "\n",
    "####################################\n",
    "#####Setting before training######\n",
    "####################################\n",
    "Start_new_training=True\n",
    "Load_from_driver=False\n",
    "Keep_training=True\n",
    "\n",
    "#####Model loading\n",
    "if Start_new_training:\n",
    "    torch.cuda.empty_cache()\n",
    "    model=BertModel()\n",
    "    writer=SummaryWriter(log_dir=Work_path+'runs')\n",
    "    writer_epochs=0\n",
    "    \n",
    "\n",
    "\n",
    "if Load_from_driver:\n",
    "    torch.cuda.empty_cache()\n",
    "    model=BertModel()\n",
    "    model.load_state_dict(torch.load(Path))\n",
    "\n",
    "\n",
    "##########################################\n",
    "#################Dataloading##############\n",
    "##########################################\n",
    "df = df[0:1000]    #we pick only 1000 example\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), [int(.8 * len(df)), int(.9 * len(df))])   \n",
    "\n",
    "\n",
    "\n",
    "##############Parameter setting##########\n",
    "LEARNING_RATE = 0.5e-2   \n",
    "EPOCHS = 3\n",
    "accumulation_steps=8    #with accumutlation_step bigger than 1, we can save the usage of GPU storage\n",
    "LEARNING_RATE=LEARNING_RATE*accumulation_steps  #Lr should be increased, or else the training will be too slow\n",
    "permit_decrease=5\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "Unknow_label=8\n",
    "model.crf=True\n",
    "#############Start training###########\n",
    "if Keep_training:\n",
    "    epochs_trained=train_loop(model, df_train, df_val,optimizer=optimizer,EPOCHS=EPOCHS,accumulation_steps=accumulation_steps,permit_decrease=permit_decrease,Unknow_label=Unknow_label,writer=writer,writer_epochs=writer_epochs)\n",
    "    writer_epochs+=epochs_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Model sava to input \n",
    "#torch.save(model.state_dict(), Path)\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8s25EGGP4iA"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.119\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, df_test):\n",
    "    test_dataset = DataSequence(df_test,labels_to_ids,label_all_tokens)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "    #for local or cloud\n",
    "    use_cuda = True\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0.0\n",
    "    total_nb_valid_example=0\n",
    "\n",
    "    for test_data, test_label in test_dataloader:\n",
    "        test_label = test_label.to(device)\n",
    "        mask = test_data['attention_mask'].to(device)\n",
    "        input_id = test_data['input_ids'].to(device)\n",
    "        \n",
    "        input_id_sqe=torch.squeeze(input_id)\n",
    "        mask_sqe=torch.squeeze(mask)\n",
    "        \n",
    "        loss, logits = model(input_id_sqe, mask_sqe, test_label)\n",
    "        \n",
    "        acc,nb_valid_example=acc_calculation(logits,test_label,Unknow_label)\n",
    "              \n",
    "        total_acc_test += acc\n",
    "        total_nb_valid_example+=nb_valid_example\n",
    "\n",
    "    val_accuracy = total_acc_test / total_nb_valid_example\n",
    "    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
    "\n",
    "\n",
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "\n",
    "test_dataset = DataSequence(df_test,labels_to_ids,label_all_tokens)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "\n",
    "test_data, test_label=next(iter(test_dataloader))\n",
    "\n",
    "mask = test_data['attention_mask'].to(device)\n",
    "input_id = test_data['input_ids'].to(device)\n",
    "        \n",
    "input_id_sqe=torch.squeeze(input_id)\n",
    "mask_sqe=torch.squeeze(mask)\n",
    "\n",
    "loss, logits = model(input_id_sqe, mask_sqe, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(num_tags=10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crf_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 9])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_length = 3  # maximum sequence length in a batch\n",
    "# batch_size = 2  # number of samples in the batch\n",
    "# emissions = torch.randn(seq_length, batch_size, num_tags)\n",
    "# tags = torch.tensor([[0, 1], [2, 4], [3, 1]], dtype=torch.long)  # (seq_length, batch_size)\n",
    "\n",
    "# mask = torch.tensor([[1, 0], [0, 1], [0, 0]], dtype=torch.uint8)\n",
    "# model_crf(emissions, tags,mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 511])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_adapted_first_end(mask):\n",
    "    \"\"\"\n",
    "    To adapt the output of bert to input of crf:\n",
    "\n",
    "    Here we delete the last two '1' in the end\n",
    "    1 is for the disappear of first first token \n",
    "    and 1 is for imforming the ML that the last one is not important\n",
    "     \"\"\"\n",
    "    mask_squ=copy.deepcopy(mask)\n",
    "    label_batch_filted=[]\n",
    "    for label_win in mask_squ:\n",
    "        #print('before',(label_win!=0).sum())\n",
    "        label_win[((label_win!=0).sum()-1)]=0\n",
    "        label_win[((label_win!=0).sum()-2)]=0\n",
    "        #print('after',(label_win!=0).sum())\n",
    "        label_batch_filted.append(label_win)\n",
    "        mask_filted=torch.stack(label_batch_filted)\n",
    "        \n",
    "    return torch.index_select(mask_filted,1,torch.IntTensor(range(511))).type(torch.bool)\n",
    "\n",
    "mask_adapted_first_end(mask_sqe).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_label(test_label):\n",
    "    \"\"\"\n",
    "    To adapt the output of bert to input of crf:\n",
    "    1, cut off the first label of each sentence, which is always -100\n",
    "    2, replace all -100 by 8, which will be ignored thanks to mask, (we have to do it or else the program cann't go on)\n",
    "    \"\"\"\n",
    "    delete_first_col_label=torch.index_select(test_label,1,1+torch.IntTensor(range(511)))    \n",
    "    label_change_value=torch.where(delete_first_col_label==-100,torch.tensor(Unknow_label),delete_first_col_label)  \n",
    "    return label_change_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_logits_first(logits):\n",
    "    \"\"\"\n",
    "    To adapt the output of bert to input of crf:\n",
    "    1, We delete the prediction of first token in each sentence,since we do so in filter_label\n",
    "    \"\"\"\n",
    "    return torch.index_select(logits,1,1+torch.IntTensor(range(511)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 8, 8,  ..., 8, 8, 8],\n",
       "        [8, 8, 8,  ..., 8, 8, 8],\n",
       "        [2, 6, 6,  ..., 8, 8, 8],\n",
       "        [8, 8, 8,  ..., 8, 8, 8]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_label(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_adapted_first_end(mask_sqe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÂØπmask‰∏çÊä±ÂπªÊÉ≥ÔºåÁõ¥Êé•Â§ÑÁêÜÂíålabel,Êää-100ÊîπÊàê10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-71.0673, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crf_layer(filter_logits_first(logits),filter_label(test_label),mask_adapted_first_end(mask_sqe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2RcPF91P7FS",
    "outputId": "6ff4203e-77da-46d4-c208-85c315873add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âº†‰∏âÂ∑≤ÁªèÂà∞ËæΩÂÆÅÊ≤àÈò≥‰∫ÜÂó∑ÔºåÊùéÂç°ÊåáÂÆöÂú®‰πùÊúà‰∫îÊó•Ê≤°ÊúâÂ•ΩÊûúÊ±ÅÂêÉ\n",
      "['B_PER', 'I_PER', 'O', 'O', 'O', 'B_LOC', 'I_LOC', 'I_LOC', 'I_LOC', 'O', 'O', 'O', 'B_PER', 'I_PER', 'O', 'O', 'O', 'B_T', 'I_T', 'I_T', 'I_T', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "def align_word_ids(texts):\n",
    "  \n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=Max_len, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1 if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "\n",
    "def evaluate_one_text(model, sentence):\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = Max_len, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    mask = text['attention_mask'][0].unsqueeze(0).to(device)\n",
    "\n",
    "    input_id = text['input_ids'][0].unsqueeze(0).to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(input_id, mask, None)\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "\n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
    "    print(sentence)\n",
    "    print(prediction_label)\n",
    "            \n",
    "evaluate_one_text(model, 'Âº†‰∏âÂ∑≤ÁªèÂà∞ËæΩÂÆÅÊ≤àÈò≥‰∫ÜÂó∑ÔºåÊùéÂç°ÊåáÂÆöÂú®‰πùÊúà‰∫îÊó•Ê≤°ÊúâÂ•ΩÊûúÊ±ÅÂêÉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊùÄÈ©¨ÁâπÂõ¢ÈïøÔºåÊàëÂà∞Ê≤àÈò≥‰∫ÜÔºå‰Ω†Âíå‰Ω†ÂæíÂºüÂë¢\n",
      "['O', 'I_PER', 'I_PER', 'O', 'O', 'O', 'O', 'O', 'B_LOC', 'I_LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "evaluate_one_text(model, 'ÊùÄÈ©¨ÁâπÂõ¢ÈïøÔºåÊàëÂà∞Ê≤àÈò≥‰∫ÜÔºå‰Ω†Âíå‰Ω†ÂæíÂºüÂë¢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ja67f609MuJr"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d07b84835cd9e344cfc6b36587121331cd748b79cb07c96e023b0204f3468bf8"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b30e1f9dd8145dc896b42cf5f6e75db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "18619a87d26f44769572d9b86fe27b2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1b34f2edb3a4b1db14f0771065d7142",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1a59fe7300b2425e8f8022516e2596ff",
      "value": " 110k/110k [00:00&lt;00:00, 267kB/s]"
     }
    },
    "1895bcecaad248f1a3629ca628c0113d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f80873f4dc5245f1acbd5dcd06e682d2",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5e94c4476ad04f4680e1a76eaa563cca",
      "value": " 29.0/29.0 [00:00&lt;00:00, 833B/s]"
     }
    },
    "1a59fe7300b2425e8f8022516e2596ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d6de102451b4d95962167f504afcd08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21a253b6aba54884bc6c461e2aca5638": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "26440bb6e9ec49949fb645db9ef4ea97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62fa584e4b8f4699bea1479002ff9f81",
      "max": 624,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_701f66158a644085a7648fd5279006d2",
      "value": 624
     }
    },
    "2e8f932a01244043924c9270c7723e88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b19e14b78e844adc9068ea51cef7faf9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_1d6de102451b4d95962167f504afcd08",
      "value": " 624/624 [00:00&lt;00:00, 24.2kB/s]"
     }
    },
    "2f738a04f2b64ad3b3769c5e2bfa6947": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec495c84fa9a45e5ac826c004d41ef81",
      "max": 268943,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b30e1f9dd8145dc896b42cf5f6e75db",
      "value": 268943
     }
    },
    "2f85a6c8c3e94cc2ac9a7ceb2117c84e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd8efd9f3b844144b3d20bacb0ce1cee",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f6201b0f0b00482bb50c6034380e1f98",
      "value": "Downloading: 100%"
     }
    },
    "2fd2ca56be0c40c89a18406f4ddfcee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f00e2a50058944478fda51f7fd5cfe5b",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd468a3294e94e248920887ec1fd1247",
      "value": 29
     }
    },
    "349e61635cc74c3880e1dcdc424fbefa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39b2ec0232cc4c8d9f94edbd6f6f5824": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e9c6c4b3f5849069716235ad5a59a6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "554e0596bc924337beb2e7903f1932f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d5eab144ca1346fd89d614753ad8fad0",
       "IPY_MODEL_f2e1dec4abfe4076b32567bdd67ec738",
       "IPY_MODEL_18619a87d26f44769572d9b86fe27b2f"
      ],
      "layout": "IPY_MODEL_3e9c6c4b3f5849069716235ad5a59a6c"
     }
    },
    "5e94c4476ad04f4680e1a76eaa563cca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62fa584e4b8f4699bea1479002ff9f81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "701f66158a644085a7648fd5279006d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "844fe36ff8b44ff197623646fede713f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c614b67cadb9465492de63927172911f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b8a66518d8dc4853a607a968e51d2cea",
      "value": " 269k/269k [00:00&lt;00:00, 1.09MB/s]"
     }
    },
    "927a24da570342b093add93c64406361": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0d88595282e4d3c9cbb558f39826fa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf106b03f01f43378a87170bf130cb9b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_349e61635cc74c3880e1dcdc424fbefa",
      "value": "Downloading: 100%"
     }
    },
    "b19e14b78e844adc9068ea51cef7faf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1b34f2edb3a4b1db14f0771065d7142": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3c99eb84abb410d9888f87631803b22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8a66518d8dc4853a607a968e51d2cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf106b03f01f43378a87170bf130cb9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c093c6bfe8ab4b69afdb8e63a7a72819": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3f82e10e0ea4231b254d68d190325f4",
       "IPY_MODEL_2f738a04f2b64ad3b3769c5e2bfa6947",
       "IPY_MODEL_844fe36ff8b44ff197623646fede713f"
      ],
      "layout": "IPY_MODEL_f0c00fa682254c1fbcb0f9db06b8d5b2"
     }
    },
    "c614b67cadb9465492de63927172911f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd468a3294e94e248920887ec1fd1247": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5eab144ca1346fd89d614753ad8fad0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_927a24da570342b093add93c64406361",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f84a480e089d4364bc9c2860a5e76269",
      "value": "Downloading: 100%"
     }
    },
    "dd8efd9f3b844144b3d20bacb0ce1cee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e146fc0fdad84efbbb060632723d391c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec495c84fa9a45e5ac826c004d41ef81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f00e2a50058944478fda51f7fd5cfe5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f048a8857a654458af82e4f53ada6ce5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0c00fa682254c1fbcb0f9db06b8d5b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2e1dec4abfe4076b32567bdd67ec738": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39b2ec0232cc4c8d9f94edbd6f6f5824",
      "max": 109540,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_21a253b6aba54884bc6c461e2aca5638",
      "value": 109540
     }
    },
    "f3c4a4db469d436c9ef388d018c8fd33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f85a6c8c3e94cc2ac9a7ceb2117c84e",
       "IPY_MODEL_26440bb6e9ec49949fb645db9ef4ea97",
       "IPY_MODEL_2e8f932a01244043924c9270c7723e88"
      ],
      "layout": "IPY_MODEL_f048a8857a654458af82e4f53ada6ce5"
     }
    },
    "f3f82e10e0ea4231b254d68d190325f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb8a3663a8cc40d288ecc0352baae919",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b3c99eb84abb410d9888f87631803b22",
      "value": "Downloading: 100%"
     }
    },
    "f6201b0f0b00482bb50c6034380e1f98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f80873f4dc5245f1acbd5dcd06e682d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f816a3771dca4f4ba559ac3ad41213bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a0d88595282e4d3c9cbb558f39826fa1",
       "IPY_MODEL_2fd2ca56be0c40c89a18406f4ddfcee0",
       "IPY_MODEL_1895bcecaad248f1a3629ca628c0113d"
      ],
      "layout": "IPY_MODEL_e146fc0fdad84efbbb060632723d391c"
     }
    },
    "f84a480e089d4364bc9c2860a5e76269": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb8a3663a8cc40d288ecc0352baae919": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
